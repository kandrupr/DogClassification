{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's That Dog\n",
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Conv2D, Activation, Dense, MaxPooling2D, Dropout, Flatten\n",
    "from keras.utils import to_categorical\n",
    "# import itertools\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __18180 images__\n",
    "* __1200 images__\n",
    "* __1200 images__\n",
    "* __All image sets have 120 categories / breeds of dogs__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training = os.path.join(os.getcwd(), 'assets\\\\train')\n",
    "test = os.path.join(os.getcwd(), 'assets\\\\test')\n",
    "validation = os.path.join(os.getcwd(), 'assets\\\\validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All the categories / breeds of dogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = [i for i in os.listdir(training)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Object that maps the images to the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18180 images belonging to 120 classes.\n",
      "Found 1200 images belonging to 120 classes.\n",
      "Found 1200 images belonging to 120 classes.\n"
     ]
    }
   ],
   "source": [
    "train_batches = ImageDataGenerator().flow_from_directory(training, target_size=(256,256), classes=labels, batch_size=100)\n",
    "test_batches = ImageDataGenerator().flow_from_directory(test, target_size=(256,256), classes=labels, batch_size=10)\n",
    "valid_batches = ImageDataGenerator().flow_from_directory(validation, target_size=(256,256), classes=labels, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# \n",
    "# for i in label:\n",
    "#     count = 0\n",
    "#     for j in i:\n",
    "#         if(j == 1.0):\n",
    "#             print (labels[count])\n",
    "#             break\n",
    "#         else:\n",
    "#             count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Model Architecture 3 layers 1 hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 254, 254, 32)      896       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2064512)           0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 120)               247741560 \n",
      "=================================================================\n",
      "Total params: 247,742,456\n",
      "Trainable params: 247,742,456\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(256, 256 ,3)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(120, activation='softmax'))\n",
    "\n",
    "# Basic model by deep lizard\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Number of sweeps per epoch\n",
    "epoch_steps = 10\n",
    "# Numbers of rounds\n",
    "num_epoch = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "10/10 [==============================] - 201s 20s/step - loss: 15.6000 - acc: 0.0300 - val_loss: 15.9569 - val_acc: 0.0100\n",
      "Epoch 2/5\n",
      "10/10 [==============================] - 166s 17s/step - loss: 16.1181 - acc: 0.0000e+00 - val_loss: 15.9569 - val_acc: 0.0100\n",
      "Epoch 3/5\n",
      "10/10 [==============================] - 182s 18s/step - loss: 15.9569 - acc: 0.0100 - val_loss: 15.9569 - val_acc: 0.0100\n",
      "Epoch 4/5\n",
      "10/10 [==============================] - 185s 18s/step - loss: 16.1181 - acc: 0.0000e+00 - val_loss: 15.9569 - val_acc: 0.0100\n",
      "Epoch 5/5\n",
      "10/10 [==============================] - 147s 15s/step - loss: 15.9569 - acc: 0.0100 - val_loss: 15.9569 - val_acc: 0.0100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26ed7ba3358>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(test_batches, steps_per_epoch=epoch_steps,\n",
    "                   validation_data=valid_batches, validation_steps=epoch_steps, epochs=num_epoch, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x,y = next(train_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 4s 40ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[15.956914367675781, 0.01]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x,y)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:snakes]",
   "language": "python",
   "name": "conda-env-snakes-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
